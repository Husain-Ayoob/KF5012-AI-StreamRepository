{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Testing\n",
    "The real test of a machine learning algorithm is when it is deployed and making predictions on *new*, unseen data for which the target variable is unknown. For example, when an algorithm predicts the level of disease progression for a diabetes patient with attributes not seen before.\n",
    "\n",
    "To be able us to estimate how a machine learning algorithm will perform on new data, we need to use some of the available dataset to test the algorithm after it has been trained. To do this, we split the dataset into what we refer to as a *training set* and a *test set*. In this notebook we will explore this idea further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting a dataset into training and test sets\n",
    "There are a few different ways to split a dataset into a training and test set. Some key decision are:\n",
    "* What proportion of the dataset is going to be saved for testing?\n",
    "* How will the data points be allocated to the training and test sets?\n",
    "\n",
    "The size of test set (as a proportion of the dataset) saved for testing depends on the problem and the data, but generally it will be between $[0.5,0.9]$. It will be large enough to be confident about the estimate of future performance, but small enough so that the performance of the algorithm will not change too much if we retrain on the whole dataset.\n",
    "\n",
    "The method for allocating data points to the training and test set will depend also depend on the problem and the data, but a common way to allocate datapoints is to do it by random. Let's explore what this looks like for a toy dataset for $X$.\n",
    "\n",
    "Run the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic data\n",
    "X = np.arange(20)\n",
    "print(f\"Dataset: {X}\")\n",
    "\n",
    "# Specify size of test set as fraction of dataset\n",
    "test_size=0.2\n",
    "\n",
    "# Generate training and test sets\n",
    "np.random.shuffle(X)\n",
    "n = round(len(X)*test_size)\n",
    "X_train, X_test = X[n:], X[:n]\n",
    "print(f\"Training set: {X_train}\")\n",
    "print(f\"Testing set: {X_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now try changing `test_size` and re-running the above code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating model performance on the test set\n",
    "Lucky for us, scikit learn has built in features for dealing with training and testing. Let's introduce those while brining togther all the concepts covered already to train a machine learning regression model and evaluate it on a test set of the diabetes data.\n",
    "\n",
    "Step by step:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1) Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_diabetes = pd.read_csv('Data/diabetes.tsv', sep='\\t', header=0)\n",
    "df_diabetes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(2) Format and normalise (pre-process) the data into $X$ and $y$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df_diabetes['Y']\n",
    "X = df_diabetes.drop('Y', axis = 1)\n",
    "X=(X-X.mean())/X.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(3) Split $X$ and $y$ into training and test sets and check the shapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size=0.2\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=0)\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(4) Fit a linear regression model to the training set and calculate predictions for the *test* set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lmod = LinearRegression()\n",
    "lmod.fit(X_train, y_train)\n",
    "y_pred = lmod.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(5) Use the predictions to calculate the RSME of the model on the *test* set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = (np.sqrt(mean_squared_error(y_test, y_pred)))\n",
    "print(rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(6) Plot y_test against y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame({'y_test':y_test,'y_pred':y_pred})\n",
    "results.plot.scatter(x='y_test', y='y_pred');"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
